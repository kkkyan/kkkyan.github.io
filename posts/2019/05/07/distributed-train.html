<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深度学习框架分布式训练总结 | 鸽呜咕的博客</title>
    <meta name="description" content="总会鸽呜咕的博客">
    <meta name="generator" content="VuePress 1.3.0">
    <meta property="article:published_time" content="Tue May 07 2019 08:00:00 GMT+0800 (中国标准时间)"><meta property="article:modified_time" content="Mon May 13 2019 17:11:51 GMT+0800 (中国标准时间)"><meta property="og:site_name" content="鸽呜咕的博客"><meta property="og:title" content="深度学习框架分布式训练总结"><meta property="og:type" content="website"><meta property="og:url" content="/posts/2019/05/07/distributed-train.html"><meta name="twitter:title" content="深度学习框架分布式训练总结"><meta name="twitter:url" content="/posts/2019/05/07/distributed-train.html"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:label2" content="Filed under"><meta name="twitter:data2" content="深度学习, 分布式训练"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="分布式训练">
    <link rel="preload" href="/assets/js/vendor.vue.a0164783.js" as="script"><link rel="preload" href="/assets/css/0.styles.d777db81.css" as="style"><link rel="preload" href="/assets/js/vendor.commons.d777db81.js" as="script"><link rel="preload" href="/assets/css/styles.eba4ba0f.css" as="style"><link rel="preload" href="/assets/js/app.eba4ba0f.js" as="script"><link rel="preload" href="/assets/css/5.styles.6db3e396.css" as="style"><link rel="preload" href="/assets/js/5.6db3e396.js" as="script"><link rel="preload" href="/assets/js/20.e69b1d76.js" as="script"><link rel="prefetch" href="/assets/css/1.styles.b2c0bf60.css"><link rel="prefetch" href="/assets/css/4.styles.abd55d40.css"><link rel="prefetch" href="/assets/css/6.styles.adf54e21.css"><link rel="prefetch" href="/assets/css/7.styles.59e1b331.css"><link rel="prefetch" href="/assets/css/8.styles.e7ff88b8.css"><link rel="prefetch" href="/assets/js/1.b2c0bf60.js"><link rel="prefetch" href="/assets/js/10.cd53a94b.js"><link rel="prefetch" href="/assets/js/11.bb106957.js"><link rel="prefetch" href="/assets/js/12.af9867e3.js"><link rel="prefetch" href="/assets/js/13.b14f53f1.js"><link rel="prefetch" href="/assets/js/14.9a301c2f.js"><link rel="prefetch" href="/assets/js/15.d2ccc90d.js"><link rel="prefetch" href="/assets/js/16.f7966c21.js"><link rel="prefetch" href="/assets/js/17.cd9bc917.js"><link rel="prefetch" href="/assets/js/18.0a734a0d.js"><link rel="prefetch" href="/assets/js/19.f782d789.js"><link rel="prefetch" href="/assets/js/21.2ffdbc56.js"><link rel="prefetch" href="/assets/js/22.5b0c2c7a.js"><link rel="prefetch" href="/assets/js/23.9036a32d.js"><link rel="prefetch" href="/assets/js/24.89940a5d.js"><link rel="prefetch" href="/assets/js/25.f1d25d80.js"><link rel="prefetch" href="/assets/js/26.85cdd63c.js"><link rel="prefetch" href="/assets/js/27.76894133.js"><link rel="prefetch" href="/assets/js/28.2e180cdf.js"><link rel="prefetch" href="/assets/js/29.fc5d0e32.js"><link rel="prefetch" href="/assets/js/30.e664a78b.js"><link rel="prefetch" href="/assets/js/31.611a8ff3.js"><link rel="prefetch" href="/assets/js/4.abd55d40.js"><link rel="prefetch" href="/assets/js/6.adf54e21.js"><link rel="prefetch" href="/assets/js/7.59e1b331.js"><link rel="prefetch" href="/assets/js/8.e7ff88b8.js"><link rel="prefetch" href="/assets/js/9.b62280a7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.d777db81.css"><link rel="stylesheet" href="/assets/css/styles.eba4ba0f.css"><link rel="stylesheet" href="/assets/css/5.styles.6db3e396.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="vuepress-theme-meteorlxy"><header class="header" data-v-3f78c2c8><div data-v-5e061783 data-v-3f78c2c8><nav class="navbar" data-v-5e061783><div class="container" data-v-5e061783><a href="/" class="router-link-active" data-v-5e061783><span class="navbar-site-name" data-v-5e061783>
          鸽呜咕的博客
        </span></a> <div class="navbar-toggler" data-v-5e061783><svg class="icon" style="font-size:1.2em;" data-v-5e061783 data-v-5e061783><title data-v-5e061783 data-v-5e061783>menu</title><use xlink:href="#icon-menu" data-v-5e061783 data-v-5e061783></use></svg></div> <div class="navbar-links" data-v-5e061783><div class="search-box custom-search" data-v-5e061783><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <a href="/" class="navbar-link" data-v-5e061783>
            主页
          </a><a href="/posts/" class="navbar-link" data-v-5e061783>
            文章
          </a><a href="/posts/categories/" class="navbar-link" data-v-5e061783>
            分类
          </a><a href="/posts/tags/" class="navbar-link" data-v-5e061783>
            标签
          </a></div></div></nav> <div class="navbar-holder" style="display:none;" data-v-5e061783></div></div> <div class="banner" data-v-66d98992 data-v-3f78c2c8 data-v-3f78c2c8><div class="container" data-v-66d98992><div class="center" data-v-66d98992><h1 data-v-66d98992 data-v-3f78c2c8>
          深度学习框架分布式训练总结
        </h1> <section class="post-date clearfix" data-v-66d98992 data-v-3f78c2c8><p class="post-info" data-v-66d98992 data-v-3f78c2c8><span id="/posts/2019/05/07/distributed-train.html" data-flag-title="深度学习框架分布式训练总结" class="leancloud_visitors" data-v-66d98992 data-v-3f78c2c8>
              阅读数:
              <i class="leancloud-visitors-count" data-v-66d98992 data-v-3f78c2c8>999+</i>次
            </span> <span data-v-66d98992 data-v-3f78c2c8>
              字数统计 : 3575
            </span> <span data-v-66d98992 data-v-3f78c2c8>
              阅读时间 ≈ 8.9 分钟
            </span></p> <span class="create-date" data-v-66d98992 data-v-3f78c2c8>
            发布时间 : 2019-05-07
          </span></section></div></div></div></header> <div class="container clearfix show-aside" data-v-096a6383 data-v-096a6383><aside class="aside" data-v-096a6383><div class="info-card main-div" data-v-e875420e data-v-096a6383><div class="info-card-header" data-v-e875420e><a href="/" class="router-link-active" data-v-e875420e><img src="https://oss.kkyan.cn/20190314165028.gif" alt="kky" class="info-avatar" data-v-e875420e></a></div></div> <div class="post-nav-card main-div" style="position:relative;top:0;width:0px;" data-v-096a6383><div class="post-nav-contents"><svg class="icon"><title>book</title><use xlink:href="#icon-book"></use></svg> <span>文章目录</span> <div class="post-nav-toc"><ul><li><a href="/posts/2019/05/07/distributed-train.html#分布式训练概述">分布式训练概述</a></li><li><a href="/posts/2019/05/07/distributed-train.html#一些概念定义">一些概念定义</a></li><li><a href="/posts/2019/05/07/distributed-train.html#tensorflow-分布式">TensorFlow 分布式</a><ul><li><a href="/posts/2019/05/07/distributed-train.html#mirroredstrategy">MirroredStrategy</a></li><li><a href="/posts/2019/05/07/distributed-train.html#centralstroragestrategy">CentralStrorageStrategy</a></li><li><a href="/posts/2019/05/07/distributed-train.html#multiworkermirroredstrategy">MultiWorkerMirroredStrategy</a></li><li><a href="/posts/2019/05/07/distributed-train.html#tpustrategy">TPUStrategy</a></li><li><a href="/posts/2019/05/07/distributed-train.html#parameterserverstrategy">ParameterServerStrategy</a></li></ul></li><li><a href="/posts/2019/05/07/distributed-train.html#pytorch">Pytorch</a><ul><li><a href="/posts/2019/05/07/distributed-train.html#模型并行">模型并行</a></li><li><a href="/posts/2019/05/07/distributed-train.html#dataparallel">DataParallel</a></li><li><a href="/posts/2019/05/07/distributed-train.html#distributeddataparallel">DistributedDataParallel</a></li></ul></li></ul></div></div> <div class="post-nav-comments"><svg class="icon"><title>comment</title><use xlink:href="#icon-comment"></use></svg> <a href="/posts/2019/05/07/distributed-train.html#vcomments">
      评论
    </a></div></div></aside> <main class="main" data-v-096a6383><div class="post" data-v-096a6383 data-v-096a6383><article class="main-div"><div class="post-content content content__default"><p>简单总结一下主流深度学习框架 Tensorflow 和 Pytorch 的分布式训练策略。</p> <h2 id="分布式训练概述"><a href="#分布式训练概述" class="header-anchor">#</a> 分布式训练概述</h2> <p>随着计算机算力逐渐提高，神经网络的参数量规模也逐步增大，之前一个情感分析的LSTM模型可能只需要1GB不到的显存，现在一个 BERT 起步就是 11GB，CV领域更是夸张。伴随着参数量和数据量的增大，单机单卡训练以及不能满足科研的需求，因此分布式训练逐渐进入人们视野。</p> <p>分布式训练与研发中的分布式系统相似，将资源分布在一个集群内运行，提高并行效率。具体分为两种情况：<strong>模型并行</strong> 与 <strong>数据并行</strong>。</p> <p><strong>模型并行</strong>指将模型的不同层分别运行在不同的GPU中，每个GPU只计算和更新自己负责的隐藏层，以解决模型参数量过大出现的 OOM 问题。</p> <p><strong>数据并行</strong>指在多个卡上运行同一份模型，但是输入不同的数据，实现多个 batch 同时并行计算，加快模型训练速度。数据并行根据并行计算的方式不同由分为<strong>同步并行</strong>和<strong>异步并行</strong>：</p> <ul><li>同步并行指每个卡上的数据计算完后，通过某些通信方式统一计算结果，并更新梯度，训练时间取决于计算最慢的那张卡，多卡间的数据同步策略也会影响训练速度。</li> <li>异步并行指每个卡独自进行数据计算和梯度更新，涉及模型间参数及时同步的问题（略复杂）。</li></ul> <p>另外，分布式训练还有单机多卡和多机多卡（集群）区别，单机多卡往往是单进程多卡训练，而多机多卡涉及到进程间通信问题，通常是通过TCP或MPI进行数据通信。目前大多数情况下论文训练都是通过 <strong>单机多卡的异步数据并行</strong> 进行分布式训练。</p> <p>需要注意的是，由于分布式训练需要不同卡（甚至不同进程）间进行通信，所以一个模型如果想要改成分布式训练，或多或少都需要进行代码修改，分布式训练与直接写一个普通的模型还是有一定区别的。</p> <h2 id="一些概念定义"><a href="#一些概念定义" class="header-anchor">#</a> 一些概念定义</h2> <p>在分布式中涉及一些集群间的通信方法：</p> <p><strong>scatter</strong>: 分割资源，将数据按某一维度分割成若干部分，分给集群中的终端。</p> <p><strong>broadcast</strong>: 广播资源，将数据复制给集群内的每一个终端。</p> <p><strong>gather</strong>: 聚集资源，将集群中的资源集中到某一个终端。</p> <p><strong>all_gather</strong>: 集群聚集资源，通过某种通信方式，将集群中的资源<strong>集中在每个终端</strong>。</p> <p><strong>reduce</strong>: 汇总资源，将集群中的资源<strong>以某种计算方法</strong>汇总到某个终端（默认是求和）。</p> <p><strong>all_reduce</strong>: 集群汇总资源，通过某种通信方式，将集群中的资源<strong>以某种计算方法汇总到每个终端</strong>（默认求和）。</p> <p>Pytorch中的图例很好的解释了这几种集群通信的区别：</p> <p><img src="https://oss.kkyan.cn/20190513095228.png" alt=""></p> <h2 id="tensorflow-分布式"><a href="#tensorflow-分布式" class="header-anchor">#</a> TensorFlow 分布式</h2> <blockquote><p>参考版本 TensorFlow 2.0</p></blockquote> <p>自 <code>v1.12</code> 后， tf 将自己的分布式训练进行了更新，共四种不同的分布式策略，统一使用 <code>tf.distribute</code> 进行管理。</p> <p>就在总结的过程中，<code>tf 2.0</code> 手册又更新了一个 <code>CentralStrorageStrategy</code> ，看来 TensorFlow 还是很重视分布式训练这一块功能的。</p> <h3 id="mirroredstrategy"><a href="#mirroredstrategy" class="header-anchor">#</a> MirroredStrategy</h3> <p>MirroredStrategy 是一种<strong>单机多卡同步训练方法</strong>。Mirrored 会在每张卡复制一份模型，各卡计算梯度后进行all-reduce，独立更新梯度。多卡间通信默认使用 <code>Nvidia NCCL</code>（通信方式的差异将在另外一篇博客中单独总结），每张卡计算完 tensor 后将对所有卡可见，之后进行梯度更新。</p> <p>MirroredStrategy 使用方法相对简单，以 Keras 为例，其他接口可参阅<a href="https://www.tensorflow.org/alpha/guide/distribute_strategy#using_tfdistributestrategy_with_keras" target="_blank" rel="noopener noreferrer">官方文档<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> ：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 规定使用的 GPU</span>
mirrored_strategy <span class="token operator">=</span> tf<span class="token punctuation">.</span>distribute<span class="token punctuation">.</span>MirroredStrategy<span class="token punctuation">(</span>devices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;/gpu:0&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;/gpu:1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 在分布式策略下创建模型</span>
<span class="token keyword">with</span> mirrored_strategy<span class="token punctuation">.</span>scope<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'mse'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'sgd'</span><span class="token punctuation">)</span>
<span class="token comment"># 输入数据，进行训练</span>
inputs<span class="token punctuation">,</span> targets <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>需要注意的是</strong>， MirroredStrategy 是将整个 batch 平均分给各个卡（在上面的例子中，gpu0 和 gpu1 各得到 batch_size 为 5)，因此如果想规定每个卡训练的batch数量，可以使用 <code>strategy.num_replicas_in_sync</code> 来计算整个 batch：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># Compute global batch size using number of replicas.</span>
<span class="token comment"># 每张卡得到的batch</span>
BATCH_SIZE_PER_REPLICA <span class="token operator">=</span> <span class="token number">5</span>
<span class="token comment"># 整体 batch</span>
global_batch_size <span class="token operator">=</span> <span class="token punctuation">(</span>BATCH_SIZE_PER_REPLICA <span class="token operator">*</span>
                     mirrored_strategy<span class="token punctuation">.</span>num_replicas_in_sync<span class="token punctuation">)</span>
<span class="token comment"># 另外，当batch扩大时，一般会根据batch来扩大学习率</span>
LEARNING_RATES_BY_BATCH_SIZE <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">5</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">:</span> <span class="token number">0.15</span><span class="token punctuation">}</span>
learning_rate <span class="token operator">=</span> LEARNING_RATES_BY_BATCH_SIZE<span class="token punctuation">[</span>global_batch_size<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>MirroredStrategy 是分布式策略中最简单的一种，当然它的局限性也较大，只能在单机上运行，当单机GPU数量不足时则略显乏力。</p> <p>注：其实 keras 自己有单机多卡的训练接口 <code>keras.utils.multi_gpu_model</code> ，不得不说 TensorFlow 的接口真的是“百花齐放”，一种目的可以用10种方法来实现，期待2.0能对接口有一个整齐的定义。</p> <h3 id="centralstroragestrategy"><a href="#centralstroragestrategy" class="header-anchor">#</a> CentralStrorageStrategy</h3> <p>就在总结这篇文章的第3天，TensorFlow 2.0 文档又更新了新的分布式策略 <code>CentralStrorageStrategy</code>，等于现在有5种分布式方法了。</p> <p>根据官方介绍，CentralStorageStrategy 是一种<strong>单机同步数据并行</strong>训练方法， 与 Mirrored 不同的是，它不再将变量进行复制，<strong>它将所有变量储存在CPU中，将所有计算方法拷贝到每个GPU中</strong>。但是如果只有一块GPU参与计算，所有的变量和计算方法都将存于GPU中。</p> <p>从文档解释来看，CentralStorage 中的 GPU 将只负责计算工作，它根据自身的计算方法从 CPU 中获取权重，然后将计算结果返回。变量的更新操作由 CPU 来维护（但是计算应该还是在 GPU 中，可能默认第一块 GPU 负责梯度更新），不涉及复杂的多卡通信算法，瓶颈将存在于GPU与CPU间的通信带宽与 Pytorch 的 DP 方法类似，GPU 与 CPU 间频繁的变量交换可能会对分布式效率产生影响。</p> <p>截止目前(2019.5.10)官方还未对该分布式有更多说明，API手册也没找到相关接口代码，对于其细节需要等Google更新后再进一步探究。</p> <h3 id="multiworkermirroredstrategy"><a href="#multiworkermirroredstrategy" class="header-anchor">#</a> MultiWorkerMirroredStrategy</h3> <p>以下简称 MWM, 和 Mirrored 相似，均为同步数据并行策略。MWM可以进行<strong>多机多卡的分布式训练</strong>，与最传统的 Param Servers 架构不同的是，<strong>MWM 没有参数服务器</strong>，所有的 GPU 都是 worker，所有的worker 拥有一份复制的模型，计算梯度后进行 All_reduce 然后独立更新。多卡间通信默认为自动调整(<code>AUTO</code>)，将根据卡数量和结构在<code>NCCL</code> 和 <code>RING</code> 中进行切换。(目前只有 MWM 支持 Ring-based)</p> <p>MWM 使用前需要定义 workers 列表：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># workers 数量</span>
NUM_WORKERS <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment"># 每个 workers 地址</span>
IP_ADDRS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'localhost'</span><span class="token punctuation">]</span>
<span class="token comment"># workers 端口，与地址相对应</span>
PORTS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">12345</span><span class="token punctuation">]</span>
<span class="token comment"># 将配置写入环境变量</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'TF_CONFIG'</span><span class="token punctuation">]</span> <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">'cluster'</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">'worker'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'%s:%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>IP_ADDRS<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">,</span> PORTS<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>NUM_WORKERS<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">'task'</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'worker'</span><span class="token punctuation">,</span> <span class="token string">'index'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>列表中第一个定义的 worker 将作为 leader，负责变量的初始化及分发工作。(多卡通信方式的不同将对架构产生较大的影响)</p> <p>截至目前(2019.5.10), MWM会使用workers内的所有GPU（暂不能指定），并且会假定所有 workers 的GPU数量是相等的（显然现在不相等的workers会报错），未来会移除这样的假设。</p> <p>官方给了一个使用 <code>Estimator</code> 进行 MWM 训练的例子，由于本人对<code>Estimator</code>接口不是很熟悉，因此不做更多说明，有意可移步<a href="https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker" target="_blank" rel="noopener noreferrer">官方教程<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>。另外，如果想用 Keras 实现 MWM 训练，可以先关闭 eager, 然后参照 Mirrored 的例子进行训练:</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>tf<span class="token punctuation">.</span>compat<span class="token punctuation">.</span>v1<span class="token punctuation">.</span>disable_eager_execution<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>另一个事情：
Another thing to note is that when using <code>MultiWorkerMirorredStrategy</code> for multiple workers with Keras, currently <strong>the user will have to explicitly shard or shuffle the data for different workers</strong>, but we will change this in the future to automatically shard the input data intelligently.</p></blockquote> <h3 id="tpustrategy"><a href="#tpustrategy" class="header-anchor">#</a> TPUStrategy</h3> <p>TPUStrategy 顾名思义，使用 TPU 进行分布式训练。不过 TPU 资源不是人人都能接触到，就不做过多解释了，会用 TPU 的也不会对分布式训练方法感到陌生。</p> <h3 id="parameterserverstrategy"><a href="#parameterserverstrategy" class="header-anchor">#</a> ParameterServerStrategy</h3> <p>Param 是 TensorFlow 最初的分布式训练方法，它由若干个 param servers 和若干 worker servers 构成，param 用于储存参数变量，worker用于计算。 Param 严格来说是一个<strong>异步数据并行</strong>训练方法，每个 worker 自己负责参数的更新工作。</p> <p>Param 训练的过程略显复杂，需要结合 K8s 来创建不同的 param servers 和 worker servers 以及对应的 clusters（这也是 Horovod 吐槽的地方），官方似乎也在逐渐摒弃这种训练方法，移除了对应的“新手教程”，将 Param 移到了 <a href="https://github.com/tensorflow/ecosystem" target="_blank" rel="noopener noreferrer">tensorflow/ecosystem<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中，真正成为了 K8s 中的一部分。因此想要了解 Param 工作原理需要对 k8s + kubeflow 有一定了解，才能熟练运用 Param 进行训练。</p> <h2 id="pytorch"><a href="#pytorch" class="header-anchor">#</a> Pytorch</h2> <blockquote><p>参考版本：1.1.0</p></blockquote> <p>本人对 Pytorch 了解较少，因此此处的介绍更多来自于对官方文档的理解</p> <p>Pytorch 自出生后就备受好评，其动态图计算和更加python化的代码风格受到众多人追捧，其分布式计算方法也继承了 torch 简洁明了的特点，相较于 TensorFlow 复杂且没什么用处的分布式逻辑，pytorch 分布式相当简洁明了。</p> <h3 id="模型并行"><a href="#模型并行" class="header-anchor">#</a> 模型并行</h3> <p>模型并行在实际中应用较少，pytorch 使用 <code>to(device_id)</code> 方法来指定模型的位置：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">class</span> <span class="token class-name">ToyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ToyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>net1<span class="token punctuation">(</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>net2<span class="token punctuation">(</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h3 id="dataparallel"><a href="#dataparallel" class="header-anchor">#</a> DataParallel</h3> <p>Pytorch 使用 <code>nn.DataParallel(model)</code> 实现<strong>单机多卡数据并行训练</strong>：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 定义 model</span>
<span class="token keyword">class</span> <span class="token class-name">Model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token comment"># 数据</span>
batch_size <span class="token operator">=</span> <span class="token number">30</span>
data_size <span class="token operator">=</span> <span class="token number">100</span>
rand_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>OurDataSet<span class="token punctuation">,</span>batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 创建模型</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
<span class="token comment"># 开启数据并行,指定显卡</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
  model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 指定 leader，用于集中处理</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda:0&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># 运行</span>
<span class="token keyword">for</span> data <span class="token keyword">in</span> rand_loader<span class="token punctuation">:</span>
    <span class="token builtin">input</span> <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Outside: input size&quot;</span><span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&quot;output_size&quot;</span><span class="token punctuation">,</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>DataParallel 会将模型复制到每个 GPU 中，并将输入的 batch 平分给每个 GPU （例如此处batch 为30，共2张卡，则每张卡得到batch 为15），计算出最后结果后返回给 leader GPU进行汇总。DP 策略的瓶颈在于 leader GPU 的通信带宽，因为所有卡的计算结果都需要汇总到 leader，由主卡更新参数后，再将新的参数广播给所有的卡。</p> <p>DataParallel 是 Pytorch 最简单的单机多卡训练模式，也是应用最广泛的训练方法。</p> <h3 id="distributeddataparallel"><a href="#distributeddataparallel" class="header-anchor">#</a> DistributedDataParallel</h3> <p>在 DataParallel 前加一个 Distributed 就产生了新的数据并行训练方法，简称 <code>DDP</code>.  之前收集各种资料时看到有人说 DP 是用于单机多卡，而 DDP 用于多机多卡，实际不然。 DPP 相较于 DP 最明显的变化在于前面多了个 D (废话)—— 它由原先的单进程多卡模式变为了多进程模式，各个进程相互独立，由 DP 的进程内变量通信转化为了进程间的通信，从系统层面提高了通信效率。</p> <p><strong>官方强烈建议使用 DPP 进行分布式训练，即使是在单机多卡上 DDP (1 process on 1 gpu) 的表现也显著优于 DP.</strong> 我查阅了许多资料来求证为什么 DDP 的表现要优于 DP，官方文档中对于 DP 和 DDP 有这样一段描述：</p> <blockquote><p>In the single-machine synchronous case, torch.distributed or the <code>torch.nn.parallel.DistributedDataParallel()</code> wrapper may still have advantages over other approaches to data-parallelism, including <code>torch.nn.DataParallel()</code> :</p> <ul><li>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</li> <li>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</li></ul></blockquote> <p>结合我对于两种不同架构的理解，总结一下其大意是：</p> <ol><li><p>DDP 中每个进程拥有自己独立的模型和优化过程，<strong>梯度计算通过 All_Reduce 进行共享（NCCL方式）</strong>；而 DP 架构在每张卡计算结果后需要与主卡进行变量交换，主卡带宽吞吐量将成为主要瓶颈。因此 DDP 架构的通信效率会高很多。</p></li> <li><p>DDP 中每个进程独立享有CPU资源，因此在运行时更快，而 DP 中所有的worker共享了一个 GPU 资源（单进程）。</p></li></ol> <p>在实战中，DDP 运行有两种方式：</p> <ol><li><p><code>1 process on all GPUs</code></p> <p>在 1-on-all 运行模式中，每个节点（物理机）只运行一个进程，进程调用该节点的所有 GPU 资源，这与 tensorflow 里的 <code>MWM</code> 工作模式一致。</p></li> <li><p><code>1 process on 1 GPU</code></p></li></ol> <p>1-on-1 模式中一个进程只调用一个 GPU 资源，因此如果需要用N个GPU就要运行N个进程，尽管进程管理略复杂但是资源自由度最高，这也是官方最推荐的运行方式。</p> <p>本人没有深入了解过 Pytorch 开发，因此不再举例分布式训练方法，有兴趣的读者可以去官方教程（下方参考资料里也有）查看相关API介绍。</p> <hr> <p>参考资料：</p> <p>[1] <a href="https://www.tensorflow.org/alpha/guide/distribute_strategy" target="_blank" rel="noopener noreferrer">tensorflow 分布式训练向导<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[2] <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/distribute" target="_blank" rel="noopener noreferrer">tensorflow 分布式 api 手册<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[3] <a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener noreferrer">pytorch 分布式训练 DP 向导<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[4] <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener noreferrer">pytorch 分布式训练 DDP 向导<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[5] <a href="https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed" target="_blank" rel="noopener noreferrer">pytorch 分布式训练 API 手册<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[5] <a href="https://pytorch.org/docs/stable/distributed.html#distributed-basics" target="_blank" rel="noopener noreferrer">pytorch 关于 DP 与 DDP 解释<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></div></article> <section class="post-meta main-div" data-v-5e90143d><section class="post-links" data-v-5e90143d><a href="/posts/2019/03/25/summarunner.html" class="post-link" data-v-5e90143d>
      上一篇 : 论文解读 - SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents
    </a> <a href="/posts/2019/05/16/sklearn.html" class="post-link" data-v-5e90143d>
      下一篇 : 机器学习函数库总结
    </a></section></section> <section class="main-div"><div class="vcomment"><div id="vcomments"></div></div></section></div></main></div> <footer class="footer" data-v-4e0ced49><p class="footer-text" data-v-4e0ced49><span data-v-4e0ced49>Powered by </span> <a href="https://github.com/vuejs/vuepress" target="_blank" data-v-4e0ced49>
      VuePress
    </a> <span data-v-4e0ced49> | </span> <a href="https://github.com/meteorlxy/vuepress-theme-meteorlxy" target="_blank" data-v-4e0ced49>
        meteorlxy
      </a></p> <p style="display: block" data-v-4e0ced49>
    备案号: <a href="http://www.beian.miit.gov.cn" target="_blank" data-v-4e0ced49>京ICP备16040058号-1</a></p> <!----></footer></div><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/vendor.vue.a0164783.js" defer></script><script src="/assets/js/5.6db3e396.js" defer></script><script src="/assets/js/20.e69b1d76.js" defer></script><script src="/assets/js/vendor.commons.d777db81.js" defer></script><script src="/assets/js/app.eba4ba0f.js" defer></script>
  </body>
</html>
