(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{112:function(s,t,a){"use strict";a.r(t);var n=a(2),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("p",[s._v("Redis作为KV数据库，整个数据库都使用字典进行储存的，很多高级语言也实现了字典结构。")]),s._v(" "),a("h2",{attrs:{id:"字典构造"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字典构造"}},[s._v("#")]),s._v(" 字典构造")]),s._v(" "),a("h3",{attrs:{id:"hash表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hash表"}},[s._v("#")]),s._v(" Hash表")]),s._v(" "),a("p",[s._v("Redis字典低层的Hash表通过数组+单链表的方式实现：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* This is our hash table structure. Every dictionary has two of this as we\n * implement incremental rehashing, for the old to the new table. */")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 哈希表结构")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("typedef")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dictht")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    dictEntry "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// hash数组(table)的长度")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" sizemask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//掩码，见下方解析")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" used"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//已存元素个数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" dictht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 每个元素的结构")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("typedef")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dictEntry")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 键")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("union")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("val"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 值")]),s._v("\n        uint64_t u64"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        int64_t s64"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 过期时间")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("double")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dictEntry")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("next"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 指向同Hash值的下一个元素")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" dictEntry"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br")])]),a("p",[s._v("Hash表中要注意的是：")]),s._v(" "),a("ol",[a("li",[a("code",[s._v("sizemask")]),s._v("掩码：用于计算键的索引。Hash计算结果需要和数组长度取余确定最终的储存位置，为了使除法运算更加快捷，redis的hash表大小永远是4,8,16...sizemask的值永远是3,7,15……对应二进制有效位都是1，因此取余可以被"),a("strong",[s._v("二进制与")]),s._v("替换，与运算的计算速度比除法取余快得多；")]),s._v(" "),a("li",[s._v("在储存元素的时候是键值一起储存在结构体中，便于后续查找冲突时确定查找的对象（这在其他语言中也是同理）；")]),s._v(" "),a("li",[s._v("由于没有尾指针，所以hash在冲突时采用头插法的方式插入节点。")])]),s._v(" "),a("h3",{attrs:{id:"字典"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字典"}},[s._v("#")]),s._v(" 字典")]),s._v(" "),a("p",[s._v("字典结构是对Hash表的再次封装，便于应用时进行扩容缩容、遍历等操作：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("typedef")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dict")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    dictType "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("type"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 该字典对应的一些特定操作函数")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("privdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 该字典依赖的数据")]),s._v("\n    dictht ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 两个hash表")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// rehash标识")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" iterators"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 访问的迭代器数量")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[a("code",[s._v("dictType")]),s._v("是对字典操作的一些抽象，如键复制函数、值复制函数等，类似于接口，Redis多个地方都用到了字典结构，不同地方的具体方法不同，因此抽象出操作函数，搭配"),a("code",[s._v("privdata")]),s._v("一同使用。")]),s._v(" "),a("p",[a("code",[s._v("rehashidx")]),s._v("和"),a("code",[s._v("iterators")]),s._v("都是搭配字典迭代器使用的，在之后的字典迭代部分有详细讲述。")]),s._v(" "),a("h2",{attrs:{id:"字典相关操作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字典相关操作"}},[s._v("#")]),s._v(" 字典相关操作")]),s._v(" "),a("h3",{attrs:{id:"_1-添加元素"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-添加元素"}},[s._v("#")]),s._v(" 1. 添加元素")]),s._v(" "),a("p",[s._v("字典的主要操作都在ht[0]进行，如果字典发生了扩容/缩容(rehash)，则会将操作放至ht[1]。")]),s._v(" "),a("h3",{attrs:{id:"_2-字典扩容-缩容"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-字典扩容-缩容"}},[s._v("#")]),s._v(" 2. 字典扩容/缩容")]),s._v(" "),a("p",[s._v("初始化的字典容量为4，如果字典负载过大，则会发生扩容，此时字典会进入rehash，将size扩大若干倍（不一定是1倍，但大小一定是4、8、16...，直到装下所有元素）。在扩容过程中，会在ht[1]申请新的hash表，并将数据逐步插入新的表中：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[s._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[s._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("_dictExpandIfNeeded")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v('/* If we reached the 1:1 ratio, and we are allowed to resize the hash\n     * table (global setting) or we should avoid it but the ratio between\n     * elements/buckets is over the "safe" threshold, we resize doubling\n     * the number of buckets. */')]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("used "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("size "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("&&")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dict_can_resize "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("||")]),s._v("\n         d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("used"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("size "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" dict_force_resize_ratio"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("dictExpand")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("used"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\t\t\n  \t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 其他内容省略")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Expand or create the hash table */")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("dictExpand")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* the size is invalid if it is smaller than the number of\n     * elements already inside the hash table */")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("dictIsRehashing")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("||")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("used "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" DICT_ERR"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    dictht n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* the new hash table */")]),s._v("\n  \t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 这里决定了下一个大小是多少")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("unsigned")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" realsize "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("_dictNextPower")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n\t\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 中间一些判断省略")]),s._v("\n  \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Prepare a second hash table for incremental rehashing */")]),s._v("\n    d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n  \t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 标记进入rehash")]),s._v("\n    d"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("->")]),s._v("rehashidx "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" DICT_OK"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br")])]),a("p",[s._v("同理，当字典的使用率不足10%时，会发生缩容，同样会创建新的hash表，进行rehash操作。")]),s._v(" "),a("h3",{attrs:{id:"_3-渐进式rehash机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-渐进式rehash机制"}},[s._v("#")]),s._v(" 3. 渐进式rehash机制")]),s._v(" "),a("p",[s._v("rehash操作主要是将原hash表的每个元素提取出来，重新进行hash计算，插入到新的hash表中，在完成数据转移后将工作表ht[0]指向新的hash表。")]),s._v(" "),a("p",[s._v("redis字典储存着大量的数据，当数据库中键值对数量达到了百万、千万、亿级别时，整个rehash过程将非常缓慢，如果不优化rehash过程，可能会造成很严重的服务不可用现象。因此，redis采用分治思想，引入了渐进式rehash机制。")]),s._v(" "),a("p",[s._v("字典在执行插入、删除、查找、修改等操作前，都先判断当前字典rehash操作是否在进行中，进行中则调用"),a("code",[s._v("dictRehashStep")]),s._v("函数进行rehash操作（每次只对1个节点进行rehash操作，共执行1次）。除这些操作之外，当服务空闲时，如果当前字典也需要进行rehsh操作，则会调用"),a("code",[s._v("incrementallyRehash")]),s._v("函数进行批量rehash操作（每次对100个节点进行rehash操作，共执行1毫秒）。在经历N次rehash操作后，整个ht[0]的数据都会迁移到ht[1]中，这样做的好处就把是本应集中处理的时间分散到了上百万、千万、亿次操作中，所以其耗时可忽略不计。")]),s._v(" "),a("p",[s._v("在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。")]),s._v(" "),a("h2",{attrs:{id:"字典的遍历"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字典的遍历"}},[s._v("#")]),s._v(" 字典的遍历")]),s._v(" "),a("p",[s._v("普通hash表遍历很容易理解，但是由于渐进式rehash机制的存在，当一个字典处于rehash状态时，字典的遍历就可能会由于键值对的移动产生遗漏、重复遍历的情况。")]),s._v(" "),a("p",[s._v("redis在字典遍历中有两种方式："),a("code",[s._v("keys迭代器遍历")]),s._v("和"),a("code",[s._v("scan间断遍历")])]),s._v(" "),a("h3",{attrs:{id:"_1-迭代器遍历"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-迭代器遍历"}},[s._v("#")]),s._v(" 1. 迭代器遍历")]),s._v(" "),a("p",[s._v("迭代器和部分高级语言的"),a("code",[s._v("iterator")]),s._v("一致，相对于对迭代操作的高层封装，redis字典迭代器结构如下：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* If safe is set to 1 this is a safe iterator, that means, you can call\n * dictAdd, dictFind, and other functions against the dictionary even while\n * iterating. Otherwise it is a non safe iterator, and only dictNext()\n * should be called while iterating. */")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("typedef")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dictIterator")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 迭代的字典")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 当前迭代的索引")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" safe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 迭代的表以及safe标识")]),s._v("\n    dictEntry "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("entry"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("nextEntry"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//当前节点与下一个节点")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* unsafe iterator fingerprint for misuse detection. */")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" fingerprint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\t"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 字典指纹")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" dictIterator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("p",[s._v("迭代器分为"),a("strong",[s._v("普通迭代器")]),s._v("和"),a("strong",[s._v("安全迭代器")]),s._v("，普通迭代器会在迭代的过程中比对"),a("code",[s._v("fingerprint")]),s._v("字典指纹，如果指纹发生了变化，则抛出异常，因此迭代过程中不能进行如何增删改查操作（查可能也会在rehash中改变字典）。")]),s._v(" "),a("p",[s._v("安全迭代器会结合字典中的"),a("code",[s._v("iterators")]),s._v("字段，限制rehash的进行。当安全迭代器进行迭代时，会将对应字典的"),a("code",[s._v("iterators")]),s._v("字段+1，此时字典的rehash会暂停，因此迭代不会产生数据遗漏和重复遍历的情况。")]),s._v(" "),a("p",[s._v("迭代器遍历可以完整遍历一个字典，通过不同的方式保证迭代结果的正确性。但是当字典数据量很大时，迭代器会耗费大量时间，造成redis服务不可用，因此出现了类似数据库分页的间断遍历方法。")]),s._v(" "),a("h3",{attrs:{id:"_2-间断遍历"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-间断遍历"}},[s._v("#")]),s._v(" 2. 间断遍历")]),s._v(" "),a("p",[s._v("间断遍历的思想是通过设置一个游标，每次遍历一部分数据，并返回新的游标值，整个遍历过程都是围绕这个游标值的改动进行，来保证所有的数据能被遍历到。")]),s._v(" "),a("p",[s._v("显然，rehash操作会对游标遍历产生影响：")]),s._v(" "),a("p",[s._v("如果整个遍历的过程中都没有rehash影响（可能从未发生rehash，也可能rehash已经完成），redis采用了一种"),a("strong",[s._v("逆转二进制算法")]),s._v("来保证游标遍历不重复也不遗漏；")]),s._v(" "),a("p",[s._v("如果遍历的过程中正在进行rehash操作，游标会先找到两个散列表中更小的表，先对小的Hash表遍历，然后对大的Hash表遍历。如果发生了rehash，同一个元素可能会被返回多次，遍历过程中新增或者删除的Key可能会被返回，也可能不会。")]),s._v(" "),a("hr"),s._v(" "),a("p",[s._v("参考资料： 《Redis5 设计与源码分析》")])])}),[],!1,null,null,null);t.default=e.exports}}]);